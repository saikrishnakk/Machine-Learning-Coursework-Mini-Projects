{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_random_low = 0\n",
    "g_random_high = 0.01\n",
    "cols = []\n",
    "for i in range(16):\n",
    "    cols.append('col_'+str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return 1 / (1 + np.exp(-t))\n",
    "\n",
    "def sigmoid_derivative(p):\n",
    "    return p * (1 - p)    \n",
    "\n",
    "def shuffleTwoArrays(X,Y):\n",
    "    assert len(X) == len(Y)\n",
    "    p = np.random.permutation(len(X))\n",
    "    return X[p], Y[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vertical_sum(row):\n",
    "    ret = []\n",
    "    for i in range(1,17):\n",
    "        cur_sum = 0\n",
    "        for j in range(16):\n",
    "            cur_sum += row[(i + 16*j)]\n",
    "        ret.append(cur_sum/16)\n",
    "    return ret\n",
    "\n",
    "def normalize(arr,a,b):\n",
    "    mn = min(arr)\n",
    "    mx = max(arr)\n",
    "    norm_arr = []\n",
    "    for x in arr:\n",
    "        cur = x - mn\n",
    "        cur *= (b - a)\n",
    "        cur /= mx - mn\n",
    "        cur += a\n",
    "        norm_arr.append(cur)\n",
    "    return norm_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetworks:\n",
    "    def __init__(self,architecture):\n",
    "        self.n_layers = len(architecture)\n",
    "        self.architecture = architecture\n",
    "        \n",
    "        self.biases = [np.random.uniform(size=(n_neurons,1),\n",
    "                                         high=g_random_high,\n",
    "                                         low=g_random_low)\n",
    "                       for n_neurons in architecture[1:]]\n",
    "        \n",
    "        \n",
    "        self.weights = [np.random.uniform(size=(n_neurons2,n_neurons1),\n",
    "                                         high=g_random_high,\n",
    "                                         low=-g_random_low)\n",
    "                        for n_neurons1, n_neurons2 in zip(architecture[:-1], architecture[1:])]\n",
    "        \n",
    "    def resetDelta(self):\n",
    "        delta_biases = [np.zeros(b.shape) for b in self.biases]\n",
    "        delta_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "        return delta_biases,delta_weights\n",
    "    \n",
    "    def addDelta(self,Delta,delta):\n",
    "        return [D+d for D, d in zip(Delta, delta)]\n",
    "        \n",
    "    def feedForward_one_datapoint(self,x):\n",
    "        x = np.reshape(x, (-1, 1))\n",
    "        cur_act = x\n",
    "        activations = [cur_act]\n",
    "        z_values = []\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, cur_act)+b\n",
    "            z_values.append(z)\n",
    "            cur_act = sigmoid(z)\n",
    "            activations.append(cur_act)\n",
    "        return z_values,activations\n",
    "    \n",
    "    def backProp_one_datapoint(self,z_values,activations,y):\n",
    "        delta_biases,delta_weights = self.resetDelta()\n",
    "        \n",
    "        cur_delta = (activations[-1] - y) * sigmoid_derivative(z_values[-1])\n",
    "        delta_biases[-1] = cur_delta\n",
    "        delta_weights[-1] = cur_delta.dot(activations[-2].T)\n",
    "        \n",
    "        for layer in range(2,self.n_layers):\n",
    "            cur_delta = self.weights[-layer+1].T.dot(cur_delta) * sigmoid_derivative(z_values[-layer])\n",
    "            delta_biases[-layer] = cur_delta\n",
    "            delta_weights[-layer] = cur_delta.dot(activations[-layer-1].transpose())\n",
    "        return delta_biases,delta_weights\n",
    "\n",
    "    def getError(self,X,Y):\n",
    "        DELTA_biases = [np.zeros(b.shape) for b in self.biases]\n",
    "        DELTA_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x,y in zip(X,Y):\n",
    "            \n",
    "            z_values,activations = self.feedForward_one_datapoint(x)\n",
    "            delta_biases,delta_weights = self.backProp_one_datapoint(z_values,activations,y)\n",
    "            DELTA_biases = self.addDelta(DELTA_biases,delta_biases)\n",
    "            DELTA_weights = self.addDelta(DELTA_weights,delta_weights)\n",
    "        return DELTA_biases,DELTA_weights\n",
    "    \n",
    "    def gradientDescent(self,X,Y,alpha):\n",
    "        m = len(X)\n",
    "        DELTA_biases,DELTA_weights = self.getError(X,Y)\n",
    "        self.biases = [bias - (alpha/m)*error for bias,error in zip(self.biases,DELTA_biases)]\n",
    "        self.weights = [theta - (alpha/m)*error for theta,error in zip(self.weights,DELTA_weights)]\n",
    "        \n",
    "    def train(self,X,Y,alpha,maxIterations):\n",
    "        for i in range(maxIterations):\n",
    "#             print('Iteration ',i+1)\n",
    "            X,Y = shuffleTwoArrays(X,Y)\n",
    "            self.gradientDescent(X,Y,alpha)\n",
    "    \n",
    "    def predict(self,X):\n",
    "        Y = []\n",
    "        for x in X:\n",
    "            h = self.feedForward_one_datapoint(x)[1][-1][0][0]\n",
    "            if(h < 0.5):\n",
    "                Y.append(0)\n",
    "            else:\n",
    "                Y.append(1)\n",
    "\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1561, 1248, 313)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../input/data.csv',header=None,sep=' ')\n",
    "data = data.drop([257],axis=1)\n",
    "data = data.rename({0:'num'},axis=1)\n",
    "data = data.loc[data.num.isin([1.0,5.0])]\n",
    "data.num = data['num'].replace({1.0: 0.0, 5.0: 1.0})\n",
    "data = data.sample(frac=1,random_state=1).reset_index(drop=True).astype(np.float128)\n",
    "df = data.loc[0:int(0.8*len(data))-1].reset_index(drop=True)\n",
    "df_test = data.loc[int(0.8*len(data)):].reset_index(drop=True)\n",
    "len(data),len(df),len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_vals = df.iloc[:,1:257].apply(vertical_sum,axis=1)\n",
    "df[cols] = pd.DataFrame(col_vals.values.tolist(), columns=cols)\n",
    "df['variance'] = normalize(df.iloc[:,257:273].var(axis=1),-1,1)\n",
    "df['kurtosis'] = normalize(df.iloc[:,257:273].kurtosis(axis=1),-1,1)\n",
    "x = df[['kurtosis','variance']]\n",
    "y = df.num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetworks([2,10,1])\n",
    "nn.train(X=np.asarray(x),\n",
    "         Y=np.asarray(y),\n",
    "         alpha=0.1,\n",
    "         maxIterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5041707468387898,\n",
       " 0.5041669080029775,\n",
       " 0.5041712825171113,\n",
       " 0.504164651441659,\n",
       " 0.5041701832817767,\n",
       " 0.504167096961407,\n",
       " 0.5041694551365823,\n",
       " 0.5041653717866525,\n",
       " 0.5041707484335116,\n",
       " 0.5041705207920145,\n",
       " 0.5041663505919723,\n",
       " 0.5041711974039591,\n",
       " 0.5041664263828229,\n",
       " 0.5041705072228695,\n",
       " 0.5041646903173203,\n",
       " 0.5041644225811551,\n",
       " 0.5041708848933197,\n",
       " 0.5041647356876682,\n",
       " 0.5041710760545755,\n",
       " 0.5041663339335152,\n",
       " 0.5041693517629148,\n",
       " 0.5041660445202875,\n",
       " 0.5041703322510203,\n",
       " 0.5041715598549953,\n",
       " 0.5041701660134252,\n",
       " 0.5041717542384457,\n",
       " 0.5041702971240963,\n",
       " 0.5041645589829713,\n",
       " 0.5041719445137552,\n",
       " 0.504169266212253,\n",
       " 0.5041663014241344,\n",
       " 0.5041652811176884,\n",
       " 0.504169865332357,\n",
       " 0.504170915179803,\n",
       " 0.5041708034342043,\n",
       " 0.5041714563216644,\n",
       " 0.5041705455384897,\n",
       " 0.5041655197331828,\n",
       " 0.5041702743869195,\n",
       " 0.5041646292312382,\n",
       " 0.5041719891031972,\n",
       " 0.5041710999175016,\n",
       " 0.5041648297129069,\n",
       " 0.5041702356986204,\n",
       " 0.5041709579387883,\n",
       " 0.5041705046032141,\n",
       " 0.5041658206858455,\n",
       " 0.5041660737284825,\n",
       " 0.5041674225563597,\n",
       " 0.5041672967536296,\n",
       " 0.5041702165741846,\n",
       " 0.5041701761171808,\n",
       " 0.5041706736996137,\n",
       " 0.5041708217556577,\n",
       " 0.5041693056114923,\n",
       " 0.5041706185906976,\n",
       " 0.504171262892157,\n",
       " 0.5041652023790648,\n",
       " 0.504165806865327,\n",
       " 0.5041642452805849,\n",
       " 0.504167994891004,\n",
       " 0.5041709268877484,\n",
       " 0.5041667104466154,\n",
       " 0.5041706570229897,\n",
       " 0.504170976138951,\n",
       " 0.5041702055368715,\n",
       " 0.5041663861381254,\n",
       " 0.5041707220326316,\n",
       " 0.5041710389523961,\n",
       " 0.5041711741143521,\n",
       " 0.504168606030515,\n",
       " 0.504171592154288,\n",
       " 0.5041713719071946,\n",
       " 0.5041705112886036,\n",
       " 0.5041658115025351,\n",
       " 0.5041701268815418,\n",
       " 0.504171562565335,\n",
       " 0.504164349927862,\n",
       " 0.5041710362951451,\n",
       " 0.5041717047076416,\n",
       " 0.5041715308895357,\n",
       " 0.5041657100054487,\n",
       " 0.5041699878234147,\n",
       " 0.5041653476309677,\n",
       " 0.5041709663740298,\n",
       " 0.5041713906325289,\n",
       " 0.5041657411838715,\n",
       " 0.5041716221239397,\n",
       " 0.5041718963038759,\n",
       " 0.5041704723707447,\n",
       " 0.5041713235028498,\n",
       " 0.5041695451444393,\n",
       " 0.5041714885099718,\n",
       " 0.5041648191083249,\n",
       " 0.5041703497463632,\n",
       " 0.5041714398331009,\n",
       " 0.5041706118159751,\n",
       " 0.5041710224813487,\n",
       " 0.5041657335351036,\n",
       " 0.5041704026144136,\n",
       " 0.504171183247054,\n",
       " 0.5041707787514681,\n",
       " 0.5041676422345459,\n",
       " 0.5041655024738652,\n",
       " 0.5041702009568285,\n",
       " 0.5041713985279814,\n",
       " 0.5041649616098369,\n",
       " 0.504170612268806,\n",
       " 0.5041658502932906,\n",
       " 0.5041724534390417,\n",
       " 0.5041707113930983,\n",
       " 0.504169697604369,\n",
       " 0.5041693272692703,\n",
       " 0.5041657616738814,\n",
       " 0.5041703170139405,\n",
       " 0.5041709773076085,\n",
       " 0.5041648015944938,\n",
       " 0.5041710138981222,\n",
       " 0.5041715553445579,\n",
       " 0.5041707822756362,\n",
       " 0.5041703601930213,\n",
       " 0.5041698507807372,\n",
       " 0.5041638186591656,\n",
       " 0.5041648771841448,\n",
       " 0.504170787998768,\n",
       " 0.504164933534854,\n",
       " 0.504171438309151,\n",
       " 0.5041708359620974,\n",
       " 0.5041705206182245,\n",
       " 0.5041654702178329,\n",
       " 0.5041712909069365,\n",
       " 0.5041697609695026,\n",
       " 0.5041647950341184,\n",
       " 0.5041656556842784,\n",
       " 0.5041717105302658,\n",
       " 0.5041709633690585,\n",
       " 0.5041701030649087,\n",
       " 0.504169047587915,\n",
       " 0.5041655369237817,\n",
       " 0.5041711416355908,\n",
       " 0.5041709739708843,\n",
       " 0.5041661622284351,\n",
       " 0.5041707045766441,\n",
       " 0.5041711306541392,\n",
       " 0.5041695900884012,\n",
       " 0.5041703304987248,\n",
       " 0.5041637500380205,\n",
       " 0.5041650524706467,\n",
       " 0.5041710718978677,\n",
       " 0.5041709668659863,\n",
       " 0.5041705573060583,\n",
       " 0.5041716352142378,\n",
       " 0.5041698171850298,\n",
       " 0.5041713765136449,\n",
       " 0.50417124628243,\n",
       " 0.5041666598260156,\n",
       " 0.504170500226923,\n",
       " 0.5041712493710555,\n",
       " 0.5041649966372909,\n",
       " 0.5041661596280663,\n",
       " 0.5041707373555386,\n",
       " 0.50416896703152,\n",
       " 0.5041711444292329,\n",
       " 0.5041703946208947,\n",
       " 0.5041708222490393,\n",
       " 0.5041713335433938,\n",
       " 0.5041639898448327,\n",
       " 0.5041708976604976,\n",
       " 0.5041718828946469,\n",
       " 0.5041705024663533,\n",
       " 0.5041714215810144,\n",
       " 0.5041655337217293,\n",
       " 0.5041700865417946,\n",
       " 0.504169984664931,\n",
       " 0.504170052932873,\n",
       " 0.5041704353200507,\n",
       " 0.5041710137675577,\n",
       " 0.5041650703586585,\n",
       " 0.5041714942897376,\n",
       " 0.5041712090904108,\n",
       " 0.504170514628005,\n",
       " 0.5041655316208088,\n",
       " 0.5041710282868502,\n",
       " 0.5041711816152636,\n",
       " 0.50416538646883,\n",
       " 0.5041644282144853,\n",
       " 0.5041702050364586,\n",
       " 0.5041658320478888,\n",
       " 0.5041657468369729,\n",
       " 0.504164817285028,\n",
       " 0.5041711906422505,\n",
       " 0.5041704041830607,\n",
       " 0.5041705245080891,\n",
       " 0.5041661329287118,\n",
       " 0.5041670958270712,\n",
       " 0.5041723457076194,\n",
       " 0.5041702002469515,\n",
       " 0.5041710270034064,\n",
       " 0.5041704374535927,\n",
       " 0.5041694579800022,\n",
       " 0.5041710060513395,\n",
       " 0.5041710387406589,\n",
       " 0.5041654142756509,\n",
       " 0.5041661485306185,\n",
       " 0.5041644104607479,\n",
       " 0.5041716641438485,\n",
       " 0.5041655172855234,\n",
       " 0.5041641386142742,\n",
       " 0.5041661230897525,\n",
       " 0.5041652865834524,\n",
       " 0.5041647363466359,\n",
       " 0.5041659292396218,\n",
       " 0.5041649246460235,\n",
       " 0.5041671324949387,\n",
       " 0.5041685477000918,\n",
       " 0.5041704393408922,\n",
       " 0.5041655415784773,\n",
       " 0.5041711430330353,\n",
       " 0.5041696340763955,\n",
       " 0.5041711941567258,\n",
       " 0.5041657581113027,\n",
       " 0.5041725191943551,\n",
       " 0.5041649198083982,\n",
       " 0.5041714030807394,\n",
       " 0.5041703891068333,\n",
       " 0.5041712077331891,\n",
       " 0.5041710944208871,\n",
       " 0.5041713573445659,\n",
       " 0.5041655283790104,\n",
       " 0.5041652525328834,\n",
       " 0.5041708563616439,\n",
       " 0.5041711425317675,\n",
       " 0.504163983807661,\n",
       " 0.504168289831228,\n",
       " 0.5041649576107535,\n",
       " 0.5041698511279638,\n",
       " 0.5041660833613874,\n",
       " 0.5041679108383167,\n",
       " 0.5041665513071821,\n",
       " 0.5041651629717827,\n",
       " 0.504164149298181,\n",
       " 0.5041696832888871,\n",
       " 0.5041654397859621,\n",
       " 0.5041712290087709,\n",
       " 0.5041656132796665,\n",
       " 0.5041660232124429,\n",
       " 0.5041709218871778,\n",
       " 0.5041656503440228,\n",
       " 0.5041649384415848,\n",
       " 0.5041709151817767,\n",
       " 0.5041713014799779,\n",
       " 0.504166725521765,\n",
       " 0.5041653613020888,\n",
       " 0.5041665205470794,\n",
       " 0.5041712246050967,\n",
       " 0.5041650004722201,\n",
       " 0.504171417587232,\n",
       " 0.5041712954065731,\n",
       " 0.5041707618175589,\n",
       " 0.5041704323777426,\n",
       " 0.5041714195900098,\n",
       " 0.5041712329810326,\n",
       " 0.504167606240583,\n",
       " 0.5041652643046067,\n",
       " 0.5041705525040404,\n",
       " 0.504167613890968,\n",
       " 0.5041705455781085,\n",
       " 0.5041656750704302,\n",
       " 0.5041715047114173,\n",
       " 0.5041699070654229,\n",
       " 0.5041649677014828,\n",
       " 0.504170440033751,\n",
       " 0.504164762327768,\n",
       " 0.5041657019905161,\n",
       " 0.5041716186902299,\n",
       " 0.5041691446713777,\n",
       " 0.5041708067627397,\n",
       " 0.5041715888477478,\n",
       " 0.5041657981526119,\n",
       " 0.5041662473187085,\n",
       " 0.50417135179833,\n",
       " 0.5041708924726657,\n",
       " 0.504170438170621,\n",
       " 0.5041709507198514,\n",
       " 0.5041699406112237,\n",
       " 0.5041706553169122,\n",
       " 0.5041656271670372,\n",
       " 0.5041713978083888,\n",
       " 0.5041681607377263,\n",
       " 0.5041711822246226,\n",
       " 0.5041656959746983,\n",
       " 0.5041705396529909,\n",
       " 0.5041654897736227,\n",
       " 0.5041706153364208,\n",
       " 0.5041710381625945,\n",
       " 0.5041695351921436,\n",
       " 0.5041714645144824,\n",
       " 0.5041708442825824,\n",
       " 0.5041672062350029,\n",
       " 0.5041704560201421,\n",
       " 0.50416591033665,\n",
       " 0.5041710374910237,\n",
       " 0.5041650740984702,\n",
       " 0.5041651869705479,\n",
       " 0.5041707662108529,\n",
       " 0.5041644411515743,\n",
       " 0.5041652476501537,\n",
       " 0.5041712285380893,\n",
       " 0.5041677424651484,\n",
       " 0.5041710016829649,\n",
       " 0.5041656289836389,\n",
       " 0.5041714596580026,\n",
       " 0.5041709953767546]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_vals = df_test.iloc[:,1:257].apply(vertical_sum,axis=1)\n",
    "df_test[cols] = pd.DataFrame(col_vals.values.tolist(), columns=cols)\n",
    "df_test['variance'] = normalize(df_test.iloc[:,257:273].var(axis=1),-1,1)\n",
    "df_test['kurtosis'] = normalize(df_test.iloc[:,257:273].kurtosis(axis=1),-1,1)\n",
    "x = df_test[['kurtosis','variance']]\n",
    "y = df_test.num\n",
    "nn.predict(np.asarray(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
