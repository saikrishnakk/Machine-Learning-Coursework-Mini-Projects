{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_random_low = 0\n",
    "g_random_high = 0.01\n",
    "cols = []\n",
    "for i in range(16):\n",
    "    cols.append('col_'+str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return 1 / (1 + np.exp(-t))\n",
    "\n",
    "def sigmoid_derivative(p):\n",
    "    return p * (1 - p)    \n",
    "\n",
    "def shuffleTwoArrays(X,Y):\n",
    "    assert len(X) == len(Y)\n",
    "    p = np.random.permutation(len(X))\n",
    "    return X[p], Y[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vertical_sum(row):\n",
    "    ret = []\n",
    "    for i in range(1,17):\n",
    "        cur_sum = 0\n",
    "        for j in range(16):\n",
    "            cur_sum += row[(i + 16*j)]\n",
    "        ret.append(cur_sum/16)\n",
    "    return ret\n",
    "\n",
    "def normalize(arr,a,b):\n",
    "    mn = min(arr)\n",
    "    mx = max(arr)\n",
    "    norm_arr = []\n",
    "    for x in arr:\n",
    "        cur = x - mn\n",
    "        cur *= (b - a)\n",
    "        cur /= mx - mn\n",
    "        cur += a\n",
    "        norm_arr.append(cur)\n",
    "    return norm_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetworks:\n",
    "    def __init__(self,architecture):\n",
    "        self.n_layers = len(architecture)\n",
    "        self.architecture = architecture\n",
    "        \n",
    "        self.biases = [np.random.uniform(size=(n_neurons,1),\n",
    "                                         high=g_random_high,\n",
    "                                         low=g_random_low)\n",
    "                       for n_neurons in architecture[1:]]\n",
    "        \n",
    "        \n",
    "        self.weights = [np.random.uniform(size=(n_neurons2,n_neurons1),\n",
    "                                         high=g_random_high,\n",
    "                                         low=-g_random_low)\n",
    "                        for n_neurons1, n_neurons2 in zip(architecture[:-1], architecture[1:])]\n",
    "        \n",
    "    def resetDelta(self):\n",
    "        delta_biases = [np.zeros(b.shape) for b in self.biases]\n",
    "        delta_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "        return delta_biases,delta_weights\n",
    "    \n",
    "    def addDelta(self,Delta,delta):\n",
    "        return [D+d for D, d in zip(Delta, delta)]\n",
    "        \n",
    "    def feedForward_one_datapoint(self,x):\n",
    "        x = np.reshape(x, (-1, 1))\n",
    "        cur_act = x\n",
    "        activations = [cur_act]\n",
    "        z_values = []\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, cur_act)+b\n",
    "            z_values.append(z)\n",
    "            cur_act = sigmoid(z)\n",
    "            activations.append(cur_act)\n",
    "        return z_values,activations\n",
    "    \n",
    "    def backProp_one_datapoint(self,z_values,activations,y):\n",
    "        delta_biases,delta_weights = self.resetDelta()\n",
    "        \n",
    "        cur_delta = (activations[-1] - y) * sigmoid_derivative(z_values[-1])\n",
    "        delta_biases[-1] = cur_delta\n",
    "        delta_weights[-1] = cur_delta.dot(activations[-2].T)\n",
    "        \n",
    "        for layer in range(2,self.n_layers):\n",
    "            cur_delta = self.weights[-layer+1].T.dot(cur_delta) * sigmoid_derivative(z_values[-layer])\n",
    "            delta_biases[-layer] = cur_delta\n",
    "            delta_weights[-layer] = cur_delta.dot(activations[-layer-1].transpose())\n",
    "        return delta_biases,delta_weights\n",
    "\n",
    "    def getError(self,X,Y):\n",
    "        DELTA_biases = [np.zeros(b.shape) for b in self.biases]\n",
    "        DELTA_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x,y in zip(X,Y):\n",
    "            \n",
    "            z_values,activations = self.feedForward_one_datapoint(x)\n",
    "            delta_biases,delta_weights = self.backProp_one_datapoint(z_values,activations,y)\n",
    "            DELTA_biases = self.addDelta(DELTA_biases,delta_biases)\n",
    "            DELTA_weights = self.addDelta(DELTA_weights,delta_weights)\n",
    "        return DELTA_biases,DELTA_weights\n",
    "    \n",
    "    def gradientDescent(self,X,Y,alpha):\n",
    "        m = len(X)\n",
    "        DELTA_biases,DELTA_weights = self.getError(X,Y)\n",
    "        self.biases = [bias - (alpha/m)*error for bias,error in zip(self.biases,DELTA_biases)]\n",
    "        self.weights = [theta - (alpha/m)*error for theta,error in zip(self.weights,DELTA_weights)]\n",
    "        \n",
    "    def train(self,X,Y,alpha,maxIterations):\n",
    "        for i in range(maxIterations):\n",
    "#             print('Iteration ',i+1)\n",
    "            X,Y = shuffleTwoArrays(X,Y)\n",
    "            self.gradientDescent(X,Y,alpha)\n",
    "    \n",
    "    def predict(self,X):\n",
    "        Y = []\n",
    "        for x in X:\n",
    "            h = self.feedForward_one_datapoint(x)[1][-1][0][0]\n",
    "            if(h < 0.5):\n",
    "                Y.append(0)\n",
    "            else:\n",
    "                Y.append(1)\n",
    "\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1561, 1248, 313)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../input/data.csv',header=None,sep=' ')\n",
    "data = data.drop([257],axis=1)\n",
    "data = data.rename({0:'num'},axis=1)\n",
    "data = data.loc[data.num.isin([1.0,5.0])]\n",
    "data.num = data['num'].replace({1.0: 0.0, 5.0: 1.0})\n",
    "data = data.sample(frac=1,random_state=1).reset_index(drop=True).astype(np.float128)\n",
    "df = data.loc[0:int(0.8*len(data))-1].reset_index(drop=True)\n",
    "df_test = data.loc[int(0.8*len(data)):].reset_index(drop=True)\n",
    "len(data),len(df),len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_vals = df.iloc[:,1:257].apply(vertical_sum,axis=1)\n",
    "df[cols] = pd.DataFrame(col_vals.values.tolist(), columns=cols)\n",
    "df['variance'] = normalize(df.iloc[:,257:273].var(axis=1),-1,1)\n",
    "df['kurtosis'] = normalize(df.iloc[:,257:273].kurtosis(axis=1),-1,1)\n",
    "x = df[['kurtosis','variance']]\n",
    "y = df.num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetworks([2,10,1])\n",
    "nn.train(X=np.asarray(x),\n",
    "         Y=np.asarray(y),\n",
    "         alpha=0.01,\n",
    "         maxIterations=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.49999442928941845,\n",
       " 0.4999919734914087,\n",
       " 0.49999487619661803,\n",
       " 0.49999011554565437,\n",
       " 0.49999395641516803,\n",
       " 0.4999920050579317,\n",
       " 0.4999933635182015,\n",
       " 0.49999073142988376,\n",
       " 0.49999459318053574,\n",
       " 0.49999462855917476,\n",
       " 0.4999914884422623,\n",
       " 0.49999489551055915,\n",
       " 0.4999916335231882,\n",
       " 0.4999941811878079,\n",
       " 0.4999901326609519,\n",
       " 0.49998992033681033,\n",
       " 0.4999945869295788,\n",
       " 0.4999902505567159,\n",
       " 0.49999500101882355,\n",
       " 0.4999914568029876,\n",
       " 0.49999312992210637,\n",
       " 0.4999913196621817,\n",
       " 0.4999943995233599,\n",
       " 0.4999952060199648,\n",
       " 0.4999938310854352,\n",
       " 0.49999585162310445,\n",
       " 0.49999374017032266,\n",
       " 0.4999899168841222,\n",
       " 0.4999960867840129,\n",
       " 0.4999936837175437,\n",
       " 0.49999147449708997,\n",
       " 0.4999907157472756,\n",
       " 0.49999366083684,\n",
       " 0.4999947279316185,\n",
       " 0.49999496063038557,\n",
       " 0.49999517604073723,\n",
       " 0.4999942539125863,\n",
       " 0.49999070733532397,\n",
       " 0.49999402316046127,\n",
       " 0.49999010533931987,\n",
       " 0.4999961392167192,\n",
       " 0.49999473039254183,\n",
       " 0.49999033282883704,\n",
       " 0.49999442869112287,\n",
       " 0.4999947519745433,\n",
       " 0.49999431709539177,\n",
       " 0.49999063609324107,\n",
       " 0.49999123917146554,\n",
       " 0.4999923275923823,\n",
       " 0.4999923559709079,\n",
       " 0.49999398889729335,\n",
       " 0.49999424464545633,\n",
       " 0.4999949296189488,\n",
       " 0.49999401881619143,\n",
       " 0.49999321495228277,\n",
       " 0.49999424232990086,\n",
       " 0.4999949503921162,\n",
       " 0.4999906448064027,\n",
       " 0.4999911023174818,\n",
       " 0.49998976016157853,\n",
       " 0.49999279270784164,\n",
       " 0.4999944856757963,\n",
       " 0.49999187403530093,\n",
       " 0.4999949321923947,\n",
       " 0.4999949493187844,\n",
       " 0.49999393309899065,\n",
       " 0.4999916105583063,\n",
       " 0.4999943271071937,\n",
       " 0.49999497635960494,\n",
       " 0.49999485782805514,\n",
       " 0.4999932729102776,\n",
       " 0.49999581493185485,\n",
       " 0.49999508879675536,\n",
       " 0.4999941126429013,\n",
       " 0.4999911432986271,\n",
       " 0.4999939021806584,\n",
       " 0.4999951120179309,\n",
       " 0.4999898497592734,\n",
       " 0.49999471603081597,\n",
       " 0.49999523576610844,\n",
       " 0.4999955879022329,\n",
       " 0.4999910228837467,\n",
       " 0.49999403183211444,\n",
       " 0.49999076253569047,\n",
       " 0.4999950860357686,\n",
       " 0.49999497131955106,\n",
       " 0.49999088398197933,\n",
       " 0.4999952034909606,\n",
       " 0.4999959082182312,\n",
       " 0.4999942193030072,\n",
       " 0.49999518548996613,\n",
       " 0.4999935770722503,\n",
       " 0.49999556614720286,\n",
       " 0.49999031577607994,\n",
       " 0.49999415865670466,\n",
       " 0.4999955372460464,\n",
       " 0.49999439095683984,\n",
       " 0.49999475651574404,\n",
       " 0.4999910719521965,\n",
       " 0.4999940582370948,\n",
       " 0.4999948967608466,\n",
       " 0.49999452961879143,\n",
       " 0.49999260929316136,\n",
       " 0.49999082137719486,\n",
       " 0.49999445963609385,\n",
       " 0.4999952304729686,\n",
       " 0.49999034495189576,\n",
       " 0.49999410492051466,\n",
       " 0.4999907667059378,\n",
       " 0.49999493740408724,\n",
       " 0.49999443858765236,\n",
       " 0.4999935444927234,\n",
       " 0.4999933096511763,\n",
       " 0.4999910674470364,\n",
       " 0.49999429236886356,\n",
       " 0.49999479286388177,\n",
       " 0.4999902117060049,\n",
       " 0.49999480720940986,\n",
       " 0.4999956178193065,\n",
       " 0.4999945353850228,\n",
       " 0.49999410830336954,\n",
       " 0.499993673064198,\n",
       " 0.4999894068926265,\n",
       " 0.499990349453476,\n",
       " 0.4999943350269207,\n",
       " 0.4999901710962084,\n",
       " 0.4999954488668253,\n",
       " 0.4999945027861649,\n",
       " 0.49999441191825705,\n",
       " 0.4999907124829606,\n",
       " 0.49999497859255093,\n",
       " 0.4999936146954194,\n",
       " 0.4999903039046167,\n",
       " 0.4999909660043306,\n",
       " 0.49999526292707647,\n",
       " 0.49999468438806244,\n",
       " 0.49999384241981143,\n",
       " 0.49999326533912325,\n",
       " 0.4999908777661899,\n",
       " 0.49999474278270406,\n",
       " 0.49999478003348535,\n",
       " 0.4999913515964024,\n",
       " 0.4999947972678714,\n",
       " 0.4999948299398188,\n",
       " 0.4999934369823824,\n",
       " 0.4999943581853392,\n",
       " 0.4999893834867251,\n",
       " 0.49999052291229856,\n",
       " 0.4999951671294527,\n",
       " 0.4999948513061709,\n",
       " 0.4999939280820321,\n",
       " 0.49999577265010153,\n",
       " 0.49999366228902864,\n",
       " 0.49999549533376375,\n",
       " 0.49999535907700193,\n",
       " 0.499991800283103,\n",
       " 0.4999945190881189,\n",
       " 0.49999499149127985,\n",
       " 0.4999903793726699,\n",
       " 0.4999913837728503,\n",
       " 0.4999947313400447,\n",
       " 0.4999933468753628,\n",
       " 0.4999951641708896,\n",
       " 0.4999944014734577,\n",
       " 0.49999477540719683,\n",
       " 0.49999436165201655,\n",
       " 0.4999896131221735,\n",
       " 0.49999437420958676,\n",
       " 0.4999948673521715,\n",
       " 0.4999947279136358,\n",
       " 0.49999505852851583,\n",
       " 0.4999907732899989,\n",
       " 0.49999399227680696,\n",
       " 0.49999380024650597,\n",
       " 0.4999938266147525,\n",
       " 0.49999442573535785,\n",
       " 0.4999948721095644,\n",
       " 0.4999905155041964,\n",
       " 0.4999955336378548,\n",
       " 0.49999490443172395,\n",
       " 0.4999942966856807,\n",
       " 0.499990895537078,\n",
       " 0.4999951978630334,\n",
       " 0.499995163067355,\n",
       " 0.4999907320419357,\n",
       " 0.4999897989291542,\n",
       " 0.4999939378287687,\n",
       " 0.4999906302146226,\n",
       " 0.4999910904650766,\n",
       " 0.49999014730742997,\n",
       " 0.49999489928566915,\n",
       " 0.49999425218560745,\n",
       " 0.4999942846521333,\n",
       " 0.4999913591476528,\n",
       " 0.4999921536348326,\n",
       " 0.4999951202205181,\n",
       " 0.4999941836980538,\n",
       " 0.4999949231648702,\n",
       " 0.4999941786474826,\n",
       " 0.49999343460354645,\n",
       " 0.4999946744161665,\n",
       " 0.49999466977724943,\n",
       " 0.4999907487669471,\n",
       " 0.4999912554683533,\n",
       " 0.49998997135142215,\n",
       " 0.4999952701246203,\n",
       " 0.49999074013516764,\n",
       " 0.4999896411222981,\n",
       " 0.4999913651334101,\n",
       " 0.4999906807598593,\n",
       " 0.49999009351887375,\n",
       " 0.4999911348583357,\n",
       " 0.4999903732233307,\n",
       " 0.49999224765910727,\n",
       " 0.49999241262425714,\n",
       " 0.4999942665621612,\n",
       " 0.49999082551120244,\n",
       " 0.4999947772250169,\n",
       " 0.49999395226712595,\n",
       " 0.4999953699180626,\n",
       " 0.49999075046227043,\n",
       " 0.4999964888594659,\n",
       " 0.4999903629149115,\n",
       " 0.4999954360091398,\n",
       " 0.4999943622729279,\n",
       " 0.4999951004874783,\n",
       " 0.4999949463126949,\n",
       " 0.49999541040136025,\n",
       " 0.4999908084643285,\n",
       " 0.49999050347246354,\n",
       " 0.4999947108279808,\n",
       " 0.49999511588615486,\n",
       " 0.4999894634985149,\n",
       " 0.4999931907629617,\n",
       " 0.4999902773167912,\n",
       " 0.49999364259926116,\n",
       " 0.49999129456895114,\n",
       " 0.49999207565832743,\n",
       " 0.4999917064689472,\n",
       " 0.499990556575568,\n",
       " 0.49998969842272684,\n",
       " 0.4999935437736805,\n",
       " 0.49999080099881815,\n",
       " 0.4999953405963381,\n",
       " 0.4999910048752366,\n",
       " 0.4999912978972493,\n",
       " 0.49999493524551325,\n",
       " 0.49999086788777397,\n",
       " 0.49999035731212105,\n",
       " 0.4999948801673891,\n",
       " 0.4999949006261366,\n",
       " 0.4999918424771953,\n",
       " 0.499990631276229,\n",
       " 0.49999175067151874,\n",
       " 0.4999950457519784,\n",
       " 0.4999904085392148,\n",
       " 0.49999547082890333,\n",
       " 0.4999949734649099,\n",
       " 0.4999944421199998,\n",
       " 0.4999941726468661,\n",
       " 0.49999503642405313,\n",
       " 0.49999498954046634,\n",
       " 0.4999925851848984,\n",
       " 0.49999053475683003,\n",
       " 0.49999426957601495,\n",
       " 0.49999256194343134,\n",
       " 0.49999428834388987,\n",
       " 0.4999908901499023,\n",
       " 0.49999562058671376,\n",
       " 0.4999937629171732,\n",
       " 0.4999904439679654,\n",
       " 0.4999941853859523,\n",
       " 0.4999902839943473,\n",
       " 0.499990725528634,\n",
       " 0.4999957055372919,\n",
       " 0.4999932465201609,\n",
       " 0.49999452849535214,\n",
       " 0.49999523041581734,\n",
       " 0.4999910471875389,\n",
       " 0.4999914955837935,\n",
       " 0.4999950166640678,\n",
       " 0.49999492823370456,\n",
       " 0.49999417465088936,\n",
       " 0.49999466564377887,\n",
       " 0.4999940900948703,\n",
       " 0.4999944081531169,\n",
       " 0.4999909486976482,\n",
       " 0.4999950260130411,\n",
       " 0.4999930793167857,\n",
       " 0.49999493067453704,\n",
       " 0.4999910158184467,\n",
       " 0.49999461287256697,\n",
       " 0.4999907471669782,\n",
       " 0.4999947411931332,\n",
       " 0.4999946682484373,\n",
       " 0.49999335995923944,\n",
       " 0.4999946106393391,\n",
       " 0.4999945551371645,\n",
       " 0.4999923034805845,\n",
       " 0.4999943314473235,\n",
       " 0.49999121789505,\n",
       " 0.49999470238912863,\n",
       " 0.4999902920813988,\n",
       " 0.499990533811308,\n",
       " 0.49999488842582945,\n",
       " 0.49998994851531614,\n",
       " 0.4999906772176092,\n",
       " 0.4999948337638792,\n",
       " 0.4999925371929434,\n",
       " 0.49999510233465827,\n",
       " 0.4999909816682524,\n",
       " 0.499995096535635,\n",
       " 0.4999949150831784]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_vals = df_test.iloc[:,1:257].apply(vertical_sum,axis=1)\n",
    "df_test[cols] = pd.DataFrame(col_vals.values.tolist(), columns=cols)\n",
    "df_test['variance'] = normalize(df_test.iloc[:,257:273].var(axis=1),-1,1)\n",
    "df_test['kurtosis'] = normalize(df_test.iloc[:,257:273].kurtosis(axis=1),-1,1)\n",
    "x = df_test[['kurtosis','variance']]\n",
    "y = df_test.num\n",
    "nn.predict(np.asarray(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
